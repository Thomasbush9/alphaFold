{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "\n",
    "Run the cells below for the basic setup of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No colab environment, assuming local setup.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive # type: ignore\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('No colab environment, assuming local setup.')\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "    # turorials folder, e.g. 'alphafold-decoded/tutorials'\n",
    "    FOLDERNAME = None\n",
    "    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "    # Now that we've mounted your Drive, this ensures that\n",
    "    # the Python interpreter of the Colab VM can load\n",
    "    # python files from within it.\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "    %pip install py3dmol\n",
    "    %pip install modelcif\n",
    "\n",
    "    print('Connected COLAB to Google Drive.')\n",
    "\n",
    "import os\n",
    "    \n",
    "base_folder = '../model'\n",
    "control_folder = f'{base_folder}/control_values'\n",
    "\n",
    "assert os.path.isdir(control_folder), 'Folder \"control_values\" not found, make sure that FOLDERNAME is set correctly.' if IN_COLAB else 'Folder \"control_values\" not found, make sure that your root folder is set correctly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Openfold weights...\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.41.10-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting botocore==1.39.10 (from awscli)\n",
      "  Downloading botocore-1.39.10-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting docutils<=0.19,>=0.18.1 (from awscli)\n",
      "  Downloading docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from awscli)\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: PyYAML<6.1,>=3.10 in /Users/thomasbush/miniconda3/envs/alphafold/lib/python3.9/site-packages (from awscli) (6.0.2)\n",
      "Collecting colorama<0.4.7,>=0.2.5 (from awscli)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.39.10->awscli)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/thomasbush/miniconda3/envs/alphafold/lib/python3.9/site-packages (from botocore==1.39.10->awscli) (2.9.0.post0)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore==1.39.10->awscli)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/thomasbush/miniconda3/envs/alphafold/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.39.10->awscli) (1.17.0)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading awscli-1.41.10-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.39.10-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: urllib3, pyasn1, jmespath, docutils, colorama, rsa, botocore, s3transfer, awscli\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.3.0\n",
      "\u001b[2K    Uninstalling urllib3-2.3.0:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.3.0\n",
      "\u001b[2K  Attempting uninstall: docutils\n",
      "\u001b[2K    Found existing installation: docutils 0.21.2\n",
      "\u001b[2K    Uninstalling docutils-0.21.2:\n",
      "\u001b[2K      Successfully uninstalled docutils-0.21.2\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [awscli]2m8/9\u001b[0m [awscli]e]\n",
      "\u001b[1A\u001b[2KSuccessfully installed awscli-1.41.10 botocore-1.39.10 colorama-0.4.6 docutils-0.19 jmespath-1.0.1 pyasn1-0.6.1 rsa-4.7.2 s3transfer-0.13.1 urllib3-1.26.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "download: s3://openfold/openfold_params/README.txt to openfold_params/README.txt\n",
      "download: s3://openfold/openfold_params/LICENSE to openfold_params/LICENSE\n",
      "download: s3://openfold/openfold_params/finetuning_no_templ_1.pt to openfold_params/finetuning_no_templ_1.pt\n",
      "download: s3://openfold/openfold_params/finetuning_4.pt to openfold_params/finetuning_4.pt\n",
      "download: s3://openfold/openfold_params/finetuning_3.pt to openfold_params/finetuning_3.pt\n",
      "download: s3://openfold/openfold_params/finetuning_5.pt to openfold_params/finetuning_5.pt\n",
      "download: s3://openfold/openfold_params/finetuning_no_templ_2.pt to openfold_params/finetuning_no_templ_2.pt\n",
      "download: s3://openfold/openfold_params/finetuning_ptm_2.pt to openfold_params/finetuning_ptm_2.pt\n",
      "download: s3://openfold/openfold_params/finetuning_2.pt to openfold_params/finetuning_2.pt\n",
      "download: s3://openfold/openfold_params/finetuning_no_templ_ptm_1.pt to openfold_params/finetuning_no_templ_ptm_1.pt\n",
      "download: s3://openfold/openfold_params/initial_training.pt to openfold_params/initial_training.pt\n",
      "download: s3://openfold/openfold_params/finetuning_ptm_1.pt to openfold_params/finetuning_ptm_1.pt\n"
     ]
    }
   ],
   "source": [
    "# Run this to download the Openfold Weights\n",
    "import subprocess\n",
    "\n",
    "if not os.path.isdir('../model/openfold_params') or not os.listdir('../model/openfold_params'):\n",
    "    print('Download Openfold weights...')\n",
    "    %pip install awscli\n",
    "    subprocess.call(['bash', '../model/download_openfold_params.sh', '../model'])\n",
    "else:\n",
    "    print('Weights folder already exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x106dc0820>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import sys\n",
    "base_folder = Path('/Users/thomasbush/Documents/ML/alphafold-decoded/tutorials')\n",
    "\n",
    "# Remove wrong paths if any\n",
    "sys.path = [p for p in sys.path if '../model' not in p]\n",
    "\n",
    "# Add base path again\n",
    "sys.path.insert(0, str(base_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/thomasbush/Documents/ML/alphafold-decoded/tutorials/model'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We are ready. All the parts of Alphafold are implemented, and all that's left to do is to stitch them together into the full model. \n",
    "\n",
    "It is described in Algorithm 2 in the paper. Note that we omit the template stack, and we don't do ensembling. Therefore, you can omit the lines 3, 4, 19, 20, and 7-13.\n",
    "\n",
    "Go to `model.py` and implement the initialization and the forward pass. After that, check your code by running the following cell. You might have to fix some datatype missmatches that fell through earlier checks. If they come up, go back to the methods and make sure that any tensors you create in them are of the right dtype (you can grab the dtype from one of the arguments of the method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3, 4, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((12, 3, 4))\n",
    "y = torch.randn((12, 3, 4))\n",
    "\n",
    "l = [x, y]\n",
    "\n",
    "torch.stack(l, dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Iteration 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2376x2 and 18x19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_positions\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangles\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtest_module_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_positions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposition_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mangles\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mframes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/model/control_values/model_checks.py:123\u001b[0m, in \u001b[0;36mtest_module_method\u001b[0;34m(module, test_name, input_names, output_names, control_folder, method, include_batched)\u001b[0m\n\u001b[1;32m    120\u001b[0m     batched_inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(batched_inp)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 123\u001b[0m     non_batched_out \u001b[38;5;241m=\u001b[39m \u001b[43mcontrolled_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_batched_inps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_batched:\n\u001b[1;32m    125\u001b[0m         batched_out \u001b[38;5;241m=\u001b[39m controlled_execution(module, batched_inps, method)\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/model/control_values/model_checks.py:84\u001b[0m, in \u001b[0;36mcontrolled_execution\u001b[0;34m(module, inp, method)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     82\u001b[0m     param\u001b[38;5;241m.\u001b[39mcopy_(torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, param\u001b[38;5;241m.\u001b[39mnumel())\u001b[38;5;241m.\u001b[39mreshape(param\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m---> 84\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m orig_param, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(original_params, module\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m     87\u001b[0m     param\u001b[38;5;241m.\u001b[39mcopy_(orig_param)\n",
      "Cell \u001b[0;32mIn[37], line 10\u001b[0m, in \u001b[0;36mtest_method\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_method\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_positions\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangles\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/model/model.py:120\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    117\u001b[0m z \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m z_rec\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# extra msa stack\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_msa_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_msa_stack(e, z)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/feature_embedding/extra_msa_stack.py:55\u001b[0m, in \u001b[0;36mExtraMsaEmbedder.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# TODO: Pass extra_msa_feat through the linear layer defined in init.    #\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Replace \"pass\" statement with your code\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#               END OF YOUR CODE                                         #\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2376x2 and 18x19)"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "from control_values.model_checks import c_m, c_z, c_e, f_e, tf_dim, c_s, num_blocks_evoformer, num_blocks_extra_msa\n",
    "from control_values.model_checks import test_module_shape, test_module_method\n",
    "\n",
    "model = Model(c_m, c_z, c_e, f_e, tf_dim, c_s, num_blocks_extra_msa, num_blocks_evoformer)\n",
    "\n",
    "test_module_shape(model, 'model', control_folder)\n",
    "\n",
    "def test_method(*args):\n",
    "    outputs = model(*args)\n",
    "    return outputs['final_positions'], outputs['position_mask'], outputs['angles'], outputs['frames']\n",
    "\n",
    "\n",
    "test_module_method(model, 'model', 'batch', ('final_positions', 'position_mask', 'angles', 'frames'), control_folder, test_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Real Model\n",
    "\n",
    "Our model passed the check for the toy input. Now it's time to load the model in real size, with real weights, and do a real evaluation pass. For this, it's much faster to use the GPU. If you are in Colab, make sure select a GPU under \"Runtime -> Change runtime type\" if you haven't done so before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('Compatible GPU available.')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print('MPS (Metal Performance Shaders) available.')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('No compatible GPU, fallback to CPU.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the weights from Openfold for our model, as it is also written in PyTorch. However, we made some tweaks to the data without changing the actual weights (like renaming a few modules, swapping the angles so that they are predicted as (cos(phi), sin(phi)) like it's said in the paper, not (sin(phi), cos(phi))). You can look at the tweaks in `utils.py` in the method `load_openfold_weights`. \n",
    "\n",
    "If you named all your modules according to the descriptions, the following cell should run cleanly and tell you that all keys in the state dictionary matched successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import load_openfold_weights\n",
    "model = Model()\n",
    "openfold_weights = load_openfold_weights('model/openfold_params/finetuning_2.pt')\n",
    "\n",
    "model.load_state_dict(openfold_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step in the forward pass, we'll call `create_features_from_a3m` four times in a row, to get four different input batches (different because of the random selection of cluster centers) four the four different cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction.feature_extraction import create_features_from_a3m\n",
    "\n",
    "batch = None\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Compute four single batches with create_features_from_a3m.       #\n",
    "#   Then, stack all the tensors in the list to get the complete batch.   #\n",
    "##########################################################################\n",
    "\n",
    "# Replace \"pass\" statement with your code\n",
    "pass\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "expected_shapes = {\n",
    "    'msa_feat': (512, 59, 49, 4),\n",
    "    'extra_msa_feat': (5120, 59, 25, 4),\n",
    "    'target_feat': (59, 21, 4),\n",
    "    'residue_index': (59, 4),\n",
    "}\n",
    "\n",
    "shapes = { key: value.shape for key, value in batch.items() }\n",
    "assert set(expected_shapes.keys()) == set(shapes.keys())\n",
    "for key, shape in shapes.items():\n",
    "    assert expected_shapes[key] == shape, f'Shape mismatch for {key}: {shape} vs {expected_shapes[key]}'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a faster forward pass, we'll map the model and the inputs to the GPU, if a compatible one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for key, value in batch.items():\n",
    "    batch[key] = value.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for the forward pass. If you weren't careful with mapping all the tensors you created within the methods to the correct device (which you can grab from any of the input tensors of the methods), you'll have to do some debugging before this runs through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a little wrapper around the modelcif python library, to create an mmcif file from our atom positions. If you are interested, you can inspect it in `utils.py`. The following cell uses this method to create `prediction.mmcif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import to_modelcif\n",
    "from geometry.residue_constants import restypes\n",
    "\n",
    "atom_positions = outputs['final_positions'][..., -1]\n",
    "atom_mask = outputs['position_mask'][..., -1]\n",
    "seq_inds = batch['target_feat'].cpu()[..., -1].argmax(dim=-1).numpy()\n",
    "seq = ''.join([restypes[ind] for ind in seq_inds])\n",
    "\n",
    "cif_str = to_modelcif(atom_positions, atom_mask, seq)\n",
    "\n",
    "with open('model/prediction.mmcif', 'w') as f:\n",
    "    f.write(cif_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The py3Dmol library allows us to visualize our prediction in Jupyter Notebooks. You can uncomment the second `view = ...` line and comment the first one out, to display the protein prediction alongside the crystal structure from the PDB (if you do so, the little sticks in the background are the second part of the dimer from the PDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "view = py3Dmol.view()\n",
    "view = py3Dmol.view(query='2OP8')\n",
    "\n",
    "with open('model/prediction.mmcif', 'r') as f:\n",
    "    data = f.read()\n",
    "view.addModel(data, 'mmcif')\n",
    "view.addModel('2OP8')\n",
    "view.setStyle({'chain': 'A'}, {'cartoon': {'color':'spectrum'}})\n",
    "view.zoomTo()\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working in Colab, you can run the following cell to download the resulting mmcif file. Open the file in a protein view like ChimeraX on your local machine. Load the crystal structure as well. You can use the 'Tools -> Structure Analysis -> Matchmaker' to align your prediction with the template. The root mean square deviation (the error) for the alignment should be really small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're in Colab, you can run this cell to download your prediction\n",
    "if IN_COLAB:\n",
    "    from google.colab import files # type: ignore\n",
    "    files.download('model/prediction.mmcif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphafold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
