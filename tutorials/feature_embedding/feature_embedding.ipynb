{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "\n",
    "Run the cells below for the basic setup of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No colab environment, assuming local setup.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive # type: ignore\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('No colab environment, assuming local setup.')\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "    # turorials folder, e.g. 'alphafold-decoded/tutorials'\n",
    "    FOLDERNAME = None\n",
    "    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "    # Now that we've mounted your Drive, this ensures that\n",
    "    # the Python interpreter of the Colab VM can load\n",
    "    # python files from within it.\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "\n",
    "    print('Connected COLAB to Google Drive.')\n",
    "\n",
    "import os\n",
    "    \n",
    "base_folder = '../feature_embedding'\n",
    "control_folder = f'{base_folder}/control_values'\n",
    "\n",
    "assert os.path.isdir(control_folder), 'Folder \"control_values\" not found, make sure that FOLDERNAME is set correctly.' if IN_COLAB else 'Folder \"control_values\" not found, make sure that your root folder is set correctly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x104dbffa0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Embedding\n",
    "\n",
    "In this notebook, we are going to implement Alphafold's feature embedding pipeline. The process of the pipeline is visualized in the diagram below. So far, we extracted `extra_msa_feat`, `residue_index`, `target_feat` and `msa_feat` from the sequence alignment data during feature extraction. The embedding of these features is done in three steps:\n",
    "1. `InputEmbedder`: The steps in the diagram below that produce the pair representation and the MSA representation.\n",
    "\n",
    "2. `RecyclingEmbedder`: Denoted with 'R' in the diagram below. Alphafold is run multiple times, and the transformed pair and MSA representations, as well as the predicted atom positions, are fed back into the new iterations via the RecyclingEmbedder.\n",
    "\n",
    "3. `ExtraMsaStack`: The extra MSA stack feeds in additional data from the sequences in the alignment that were not selected as cluster centers. It is architecturally similar to the Evoformer, and we can reuse almost all primitives from it.\n",
    "\n",
    "Optionally, Alphafold can also use features from tempmlate protein structures. We aren't implementing this feature, that's why this part of the diagram is grayed out.\n",
    "\n",
    "<figure align=center style=\"padding: 30px\">\n",
    "<img src='images/alphafold_embedding_pipeline_grayed.png'>\n",
    "<figcaption>Source: Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583â€“589 (2021).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embedder\n",
    "\n",
    "We'll start right away by implementing the input embedder. It is described in Algorithm 3 and Algorithm 4 in the paper's supplement. Note that in Algorithm 3 Line 1, even though one linear module is written, two separate modules `linear_tf_z_i` and `linear_tf_z_j` are used for the construction of `a` and `b` respectively.\n",
    "\n",
    "Head over to `input_embedder.py`, implement the `__init__` method and check your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import sys\n",
    "base_folder = Path('/Users/thomasbush/Documents/ML/alphafold-decoded/tutorials')\n",
    "\n",
    "# Remove wrong paths if any\n",
    "sys.path = [p for p in sys.path if 'feature_embedding' not in p]\n",
    "\n",
    "# Add base path again\n",
    "sys.path.insert(0, str(base_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_embedding.control_values.embedding_checks import test_module_shape\n",
    "from feature_embedding.control_values.embedding_checks import c_m, c_z, tf_dim, msa_feat_dim\n",
    "from feature_embedding.input_embedder import InputEmbedder\n",
    "\n",
    "input_embedder = InputEmbedder(c_m, c_z, tf_dim, msa_feat_dim=msa_feat_dim, vbins=32)\n",
    "\n",
    "test_module_shape(input_embedder, 'input_embedder', control_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the diagram, the residue index is processed by `relpos` and added to the pair representation. For this, the residue index of shape (N_res,) (its value is [0,...,N_res-1]) needs to match the shape of the pair representation, which is (N_res, N_res, c_z). To get the shape (N_res, N_res), the outer difference of the residue index with itself is calculated. This results in the matrix\n",
    "$$\\begin{pmatrix} 0 & -1 & -2 & -3 & ... \\\\ 1 & 0 & -1 & -2 & ...\\\\ 2 & 1 & 0 & -1 & ...\\\\ 3 & 2 & 1 & 0 & ... \\end{pmatrix}$$\n",
    "of shape (N_res, N_res). The values are clamped to the range (-32, 32) and one-hot encoded, resulting in a tensor of shape (N_res, N_res, 2*32+1). Finally, this tensor is passed through a linear layer and gets to the shape (N_res, N_res, c_z).\n",
    "\n",
    "All of this is done in `relpos`. Implement the method and run the following cell to check your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasbush/Documents/ML/alphafold-decoded/tutorials/feature_embedding/control_values/embedding_checks.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_out = torch.load(out_file_name)\n"
     ]
    }
   ],
   "source": [
    "from feature_embedding.control_values.embedding_checks import test_module_method, c_m, c_z, tf_dim, msa_feat_dim\n",
    "\n",
    "input_embedder = InputEmbedder(c_m, c_z, tf_dim, msa_feat_dim=msa_feat_dim, vbins=32)\n",
    "\n",
    "test_module_method(input_embedder, 'input_embedder_relpos', 'residue_index', 'z_out', control_folder, lambda x: input_embedder.relpos(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`relpos` is used in the forward pass of the input embedder to embed the residue indices. Now, implement the forward pass of the input embedder according to Algorithm 3 and check your code with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (35) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfeature_embedding\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrol_values\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_checks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test_module_forward, c_m, c_z, tf_dim, msa_feat_dim\n\u001b[1;32m      3\u001b[0m input_embedder \u001b[38;5;241m=\u001b[39m InputEmbedder(c_m, c_z, tf_dim, msa_feat_dim\u001b[38;5;241m=\u001b[39mmsa_feat_dim, vbins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_module_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_embedder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_embedder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mm_out\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz_out\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/feature_embedding/control_values/embedding_checks.py:144\u001b[0m, in \u001b[0;36mtest_module_forward\u001b[0;34m(module, test_name, input_names, output_names, control_folder)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_module_forward\u001b[39m(module, test_name, input_names, output_names, control_folder):\n\u001b[0;32m--> 144\u001b[0m     \u001b[43mtest_module_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/feature_embedding/control_values/embedding_checks.py:118\u001b[0m, in \u001b[0;36mtest_module_method\u001b[0;34m(module, test_name, input_names, output_names, control_folder, method)\u001b[0m\n\u001b[1;32m    115\u001b[0m batched_inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(batched_inp)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 118\u001b[0m     non_batched_out \u001b[38;5;241m=\u001b[39m \u001b[43mcontrolled_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_batched_inps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     batched_out \u001b[38;5;241m=\u001b[39m controlled_execution(module, batched_inps, method)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(non_batched_out, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/feature_embedding/control_values/embedding_checks.py:81\u001b[0m, in \u001b[0;36mcontrolled_execution\u001b[0;34m(module, inp, method)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     79\u001b[0m     param\u001b[38;5;241m.\u001b[39mcopy_(torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, param\u001b[38;5;241m.\u001b[39mnumel())\u001b[38;5;241m.\u001b[39mreshape(param\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m---> 81\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m orig_param, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(original_params, module\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m     84\u001b[0m     param\u001b[38;5;241m.\u001b[39mcopy_(orig_param)\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/feature_embedding/control_values/embedding_checks.py:144\u001b[0m, in \u001b[0;36mtest_module_forward.<locals>.<lambda>\u001b[0;34m(*x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_module_forward\u001b[39m(module, test_name, input_names, output_names, control_folder):\n\u001b[0;32m--> 144\u001b[0m     test_module_method(module, test_name, input_names, output_names, control_folder, \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39mx: \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/alphafold/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/ML/alphafold-decoded/tutorials/feature_embedding/input_embedder.py:132\u001b[0m, in \u001b[0;36mInputEmbedder.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    129\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_tf_z_j(target_feat)\n\u001b[1;32m    131\u001b[0m z \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m b\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidue_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_msa_m(msa_feat) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_tf_m(target_feat)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#               END OF YOUR CODE                                         #\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (35) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from feature_embedding.control_values.embedding_checks import test_module_forward, c_m, c_z, tf_dim, msa_feat_dim\n",
    "\n",
    "input_embedder = InputEmbedder(c_m, c_z, tf_dim, msa_feat_dim=msa_feat_dim, vbins=32)\n",
    "\n",
    "test_module_forward(input_embedder, 'input_embedder', 'batch', ('m_out', 'z_out'), control_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recycling Embedder\n",
    "\n",
    "Alphafold is run multiple times, and the outputs of the last iteration are fed into the next one via the recycling embedder. Concretely, the first line of the MSA representation, the pair representation and the pseudo-beta positions are used. The pseudo-beta positions are the 3-dimensional coordinates of the C-beta atoms of each amino acid in the protein, as computed by the Structure Module in the last iteration. They are termed 'pseudo-beta' as the C-alpha is used for glycine. But this is something we will worry about when computing these pseudo-beta positions in the Structure Module.\n",
    "\n",
    "The Recycling Embedder is described in Algorithm 32. The implementation in the AlphaFold codebase differs a little from the described algorithm. The file `recycling_embedder.py` contains step-by-step instructions for you. Implement the init method and the forward pass, then check your code by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_embedding.control_values.embedding_checks import c_m, c_z, test_module_shape, test_module_forward\n",
    "from feature_embedding.recycling_embedder import RecyclingEmbedder\n",
    "\n",
    "recycling_embedder = RecyclingEmbedder(c_m, c_z)\n",
    "\n",
    "test_module_shape(recycling_embedder, 'recycling_embedder', control_folder)\n",
    "\n",
    "test_module_forward(recycling_embedder, 'recycling_embedder', ('m', 'z', 'x'), ('m_out', 'z_out'), control_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra MSA Stack\n",
    "\n",
    "During feature extraction, some of the sequences are randomly selected as cluster centers, and the non-selected sequences only contribute as cluster averages to the MSA feature. To incorporate additional information from these non-selected sequences, the Extra MSA Stack processes a large selection of these sequences in a less memory-intensive fashion, using so called 'global self-attention'. \n",
    "\n",
    "The Extra MSA pipeline starts of simple with a linear embedding. This is done in `ExtraMsaEmbedder` in the file `extra_msa_stack.py`. Implement the module and check your code by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_embedding.control_values.embedding_checks import test_module_shape, test_module_forward, f_e, c_e\n",
    "from feature_embedding.extra_msa_stack import ExtraMsaEmbedder\n",
    "\n",
    "extra_msa_embedder = ExtraMsaEmbedder(f_e, c_e)\n",
    "\n",
    "test_module_shape(extra_msa_embedder, 'extra_msa_embedder', control_folder)\n",
    "\n",
    "test_module_forward(extra_msa_embedder, 'extra_msa_embedder', 'batch', 'e_out', control_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This embedding is passed through the ExtraMSAStack. It is described in Algorithm 18. It is similar to the EvoformerStack, the main difference being the use of global self-attention instead of normal self-attention. Global self-attention is only using one, averaged query vector. This drastically reduced the memory consumption from $O(n^2)$ to $O(n)$, but also reduces the complexity of the outputs: The attention mechanism just generates one output vector, and it's only brought back to `N_extra` by being broadcasted during gating in the attention module.\n",
    "\n",
    "We already implemented global self-attention in our attention module, so the implementation of `MSAColumnGlobalAttention` is mostly about setting the right parameters. Check your code with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_embedding.control_values.embedding_checks import test_module_shape, test_module_forward, c_m, c_z,c,N_head\n",
    "from feature_embedding.extra_msa_stack import MSAColumnGlobalAttention\n",
    "\n",
    "msa_global_col_att = MSAColumnGlobalAttention(c_m, c_z, c, N_head)\n",
    "\n",
    "test_module_shape(msa_global_col_att, 'msa_global_col_att', control_folder)\n",
    "\n",
    "test_module_forward(msa_global_col_att, 'msa_global_col_att', 'm', 'm_out', control_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ExtraMSAStack is, just like the Evoformer, build up from a number of identical blocks. It is described in Algorithm 18, and the differences to the evoformer stack are highlighted in yellow. Implement the module `ExtraMSABlock` and check your code by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_embedding.control_values.embedding_checks import test_module_shape, test_module_forward, c_e, c_z\n",
    "from feature_embedding.extra_msa_stack import ExtraMsaBlock\n",
    "\n",
    "extra_msa_block = ExtraMsaBlock(c_e, c_z)\n",
    "\n",
    "test_module_shape(extra_msa_block, 'extra_msa_block', control_folder)\n",
    "\n",
    "test_module_forward(extra_msa_block, 'extra_msa_block', ('e', 'z'), 'm_out', control_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last up is `ExtraMsaStack`, which wraps the extra MSA blocks and executes them sequentially. Implement it and check your code with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_embedding.control_values.embedding_checks import test_module_shape, test_module_forward, c_e, c_z\n",
    "from feature_embedding.extra_msa_stack import ExtraMsaStack\n",
    "\n",
    "num_blocks = 3\n",
    "extra_msa_stack = ExtraMsaStack(c_e, c_z, num_blocks)\n",
    "\n",
    "test_module_shape(extra_msa_stack, 'extra_msa_stack', control_folder)\n",
    "\n",
    "test_module_forward(extra_msa_stack, 'extra_msa_stack', ('e', 'z'), 'm_out', control_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With that, we are finally done with the full model up to the evoformer. In theory, we are only missing the structure module now. However, implementing the Structure Module requires quite a bit of knowledge about 3D geometry: Rotations, quaternions, coordinate frames and homogenous coordinates. To get you covered, we put in a notebook on these concepts. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphafold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
